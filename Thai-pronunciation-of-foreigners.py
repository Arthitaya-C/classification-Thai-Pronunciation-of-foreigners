# -*- coding: utf-8 -*-
"""AUCC_DP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t8KamOU6IpPCcdLrQQ0sylQTi9ngNBkt

# **Import data**
"""

from google.colab import drive
drive.mount('/content/drive')

import os

data_path = 'drive/My Drive/dataset/'
print(os.listdir(data_path))

filelist = os.listdir('drive/My Drive/dataset/KR/M')

for filename in filelist:
  print(filename)

"""# **Convert .mp3 to .wav**

"""

pip install pydub

from pydub import AudioSegment
from pydub.playback import play

# filelist = os.listdir('drive/My Drive/dataset/TH/M')
# i = 0

# for filename in filelist:
#   i += 1
#   print(filename)  
#   print(i)
#   # convert wav to mp3                                                            
#   audSeg = AudioSegment.from_mp3('drive/My Drive/dataset/TH/M/'+filename)
#   audSeg.export('drive/My Drive/dataset/TH/M_w/TH_M ('+str(i)+').wav', format="wav")

# filelist = os.listdir('drive/My Drive/dataset/KR/M')
# i = 0

# for filename in filelist:
#   i += 1
#   print(filename)  
#   print(i)
#   # convert wav to mp3                                                            
#   audSeg = AudioSegment.from_mp3('drive/My Drive/dataset/KR/M/'+filename)
#   audSeg.export('drive/My Drive/dataset/KR/M_w/KR_M ('+str(i)+').wav', format="wav")

# filelist = os.listdir('drive/My Drive/dataset/KR/W')
# i = 0

# for filename in filelist:
#   i += 1
#   print(filename)  
#   print(i)
#   # convert wav to mp3                                                            
#   audSeg = AudioSegment.from_mp3('drive/My Drive/dataset/KR/W/'+filename)
#   audSeg.export('drive/My Drive/dataset/KR/W_w/KR_W ('+str(i)+').wav', format="wav")

"""# **Preprocessing**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

from pickle import dump
from pickle import load

from IPython.display import Image

import librosa
import librosa.display

from sklearn.model_selection import train_test_split

"""**Read data**


"""

dataset = data_path 
os.listdir(dataset)

"""TH"""

THW = dataset+"TH/W/TH_W (*).wav"
THM = dataset+"TH/M_w/TH_M (*).wav"

THW

THW_signals = [
    librosa.load(p)[0] for p in Path().glob(THW) 
]

THM_signals = [
    librosa.load(p)[0] for p in Path().glob(THM) 
]

len(THW_signals),len(THM_signals)

"""**JP**"""

JPW = dataset+"JP/W/JP_W (*).wav"
JPM = dataset+"JP/M/JP_M (*).wav"

JPW_signals = [
    librosa.load(p)[0] for p in Path().glob(JPW) 
]

JPM_signals = [
    librosa.load(p)[0] for p in Path().glob(JPM) 
]

len(JPW_signals),len(JPM_signals)

"""**KR**"""

KRW = dataset+"KR/W_w/KR_W (*).wav"
KRM = dataset+"KR/M_w/KR_M (*).wav"

KRW_signals = [
    librosa.load(p)[0] for p in Path().glob(KRW) 
]

KRM_signals = [
    librosa.load(p)[0] for p in Path().glob(KRM) 
]

len(KRW_signals),len(KRM_signals)

"""**CN**"""

CNW = dataset+"CN/W/CN_W (*).wav"
CNM = dataset+"CN/M/CN_M (*).wav"

CNW_signals = [
    librosa.load(p)[0] for p in Path().glob(CNW) 
]

CNM_signals = [
    librosa.load(p)[0] for p in Path().glob(CNM) 
]

len(CNW_signals),len(CNM_signals)

"""**EN**"""

ENW = dataset+"EN/W/EN_W (*).wav"
ENM = dataset+"EN/M/EN_M (*).wav"

ENW_signals = [
    librosa.load(p)[0] for p in Path().glob(ENW) 
]

ENM_signals = [
    librosa.load(p)[0] for p in Path().glob(ENM) 
]

len(ENW_signals),len(ENM_signals)

"""**Plot**

TH
"""

plt.figure(figsize=(15, 6))
for i, x in enumerate(THW_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

plt.figure(figsize=(15, 6))
for i, x in enumerate(THM_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

"""JP"""

plt.figure(figsize=(15, 6))
for i, x in enumerate(JPW_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

plt.figure(figsize=(15, 6))
for i, x in enumerate(JPM_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

"""KR"""

plt.figure(figsize=(15, 6))
for i, x in enumerate(KRW_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

plt.figure(figsize=(15, 6))
for i, x in enumerate(KRM_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

"""CN"""

plt.figure(figsize=(15, 6))
for i, x in enumerate(CNW_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

plt.figure(figsize=(15, 6))
for i, x in enumerate(CNM_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

"""EN"""

plt.figure(figsize=(15, 6))
for i, x in enumerate(ENW_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

plt.figure(figsize=(15, 6))
for i, x in enumerate(ENM_signals[:10]):
    plt.subplot(2, 5, i+1)
    librosa.display.waveplot(x[:75000])
    plt.ylim(-1, 1)

"""**Extract Feature**"""

def extract_features(signal):
    return [
        librosa.feature.zero_crossing_rate(signal)[0, 0],
        librosa.feature.spectral_centroid(signal)[0, 0],
        librosa.feature.melspectrogram(signal)[0, 0],
        librosa.feature.spectral_bandwidth(signal)[0, 0],
        librosa.feature.spectral_contrast(signal)[0, 0],
    ]

y, sr = librosa.load(librosa.ex('trumpet'))

TH_signals = THW_signals+THM_signals
JP_signals = JPW_signals+JPM_signals
KR_signals = KRW_signals+KRM_signals
CN_signals = CNW_signals+CNM_signals
EN_signals = ENW_signals+ENM_signals

TH_features = np.array([extract_features(x) for x in TH_signals])
JP_features = np.array([extract_features(x) for x in JP_signals])
KR_features = np.array([extract_features(x) for x in KR_signals])
CN_features = np.array([extract_features(x) for x in CN_signals])
EN_features = np.array([extract_features(x) for x in EN_signals])

print('TH : ',TH_features.shape)
print('JP : ',JP_features.shape)
print('KR : ',KR_features.shape)
print('CN : ',CN_features.shape)
print('EN : ',EN_features.shape)

plt.figure(figsize=(14, 5))
plt.hist(TH_features[:,0], color='b', range=(0, 0.4), alpha=0.5, bins=20)
plt.hist(JP_features[:,0], color='r', range=(0, 0.4), alpha=0.5, bins=20)
plt.hist(KR_features[:,0], color='g', range=(0, 0.4), alpha=0.5, bins=20)
plt.hist(CN_features[:,0], color='orange', range=(0, 0.4), alpha=0.5, bins=20)
plt.hist(EN_features[:,0], color='purple', range=(0, 0.4), alpha=0.5, bins=20)
plt.legend(('TH', 'JP','KR','CN','EN'))
plt.xlabel('Zero Crossing Rate')
plt.ylabel('Count')

plt.figure(figsize=(14, 5))
plt.hist(TH_features[:,1], color='b', range=(0, 8000), bins=30, alpha=0.6)
plt.hist(JP_features[:,1], color='r', range=(0, 8000), bins=30, alpha=0.6)
plt.hist(KR_features[:,1], color='g', range=(0, 8000), bins=30, alpha=0.6)
plt.hist(CN_features[:,1], color='orange', range=(0, 8000), bins=30, alpha=0.6)
plt.hist(EN_features[:,1], color='purple', range=(0, 8000), bins=30, alpha=0.6)
plt.legend(('TH', 'JP','KR','CN','EN'))
plt.xlabel('Spectral Centroid (frequency bin)')
plt.ylabel('Count')

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,10))
S_dB = librosa.power_to_db(TH_features, ref=np.max)
img = librosa.display.specshow(S_dB, x_axis='time',
                         y_axis='mel',
                         fmax=8000, ax=ax)
fig.colorbar(img, ax=ax, format='%+2.0f dB')
ax.set(title='Mel-frequency spectrogram TH')

feature_table = np.vstack((TH_features,JP_features,KR_features,CN_features,EN_features))
print("dataset :",feature_table.shape)

feature_table

scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))
data_features = scaler.fit_transform(feature_table)
print(data_features.min(axis=0))
print(data_features.max(axis=0))

plt.scatter(data_features[:200,0], data_features[:200,1], c='b')
plt.scatter(data_features[200:400,0], data_features[200:400,1], c='r')
plt.scatter(data_features[400:600,0], data_features[400:600,1], c='g')
plt.scatter(data_features[600:800,0], data_features[600:800,1], c='orange')
plt.scatter(data_features[800:1000,0], data_features[800:1000,1], c='purple')
plt.legend(('TH', 'JP','KR','CN','EN'))
plt.xlabel('Zero Crossing Rate')
plt.ylabel('Spectral Centroid')

"""# **Baseline Model**"""

from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

def create_model():
    model = Sequential()
    model.add(Dense(8, input_shape=(5,)))
    model.add(LeakyReLU(alpha=0.1))
    model.add(Dropout(0.1))
    model.add(Dense(5, activation='softmax'))
    return model

"""**Split data**

TH
"""

len(TH_features)

yTH = [0 for i in range(200)]

XTH_train,XTH_test,YTH_train,YTH_test = train_test_split(TH_features,yTH,test_size=0.2,random_state=109,shuffle=True)

len(XTH_train),len(XTH_test),len(YTH_train),len(YTH_test)

XTH_train.shape,XTH_test.shape

"""JP"""

len(JP_features)

yJP = [1 for i in range(200)]

XJP_train,XJP_test,YJP_train,YJP_test = train_test_split(JP_features,yJP,test_size=0.2,random_state=109,shuffle=True)

len(XJP_train),len(XJP_test),len(YJP_train),len(YJP_test)

XJP_train.shape,XJP_test.shape

"""KR"""

len(KR_features)

yKR = [2 for i in range(200)]

XKR_train,XKR_test,YKR_train,YKR_test = train_test_split(KR_features,yKR,test_size=0.2,random_state=109,shuffle=True)

len(XKR_train),len(XKR_test),len(YKR_train),len(YKR_test)

XKR_train.shape,XKR_test.shape

"""CN"""

len(CN_features)

yCN = [3 for i in range(200)]

XCN_train,XCN_test,YCN_train,YCN_test = train_test_split(CN_features,yCN,test_size=0.2,random_state=109,shuffle=True)

len(XCN_train),len(XCN_test),len(YCN_train),len(YCN_test)

XCN_train.shape,XCN_test.shape

"""EN"""

len(EN_features)

yEN = [4 for i in range(200)]

XEN_train,XEN_test,YEN_train,YEN_test = train_test_split(EN_features,yEN,test_size=0.2,random_state=109,shuffle=True)

len(XEN_train),len(XEN_test),len(YEN_train),len(YEN_test)

XEN_train.shape,XEN_test.shape

"""**Combine**

X_train + x_valid
"""

X_train = np.vstack((XTH_train,XJP_train,XKR_train,XCN_train,XEN_train))
print("X_train :",X_train.shape)

"""X_test"""

X_test = np.vstack((XTH_test,XJP_test,XKR_test,XCN_test,XEN_test))
print("X_test :",X_test.shape)

"""Y_train"""

Y_train = YTH_train+YJP_train+YKR_train+YCN_train+YEN_train

Y_train = to_categorical(Y_train)
print("Y_train :",Y_train.shape)

"""Y_test"""

Y_test = YTH_test+YJP_test+YKR_test+YCN_test+YEN_test

Y_test = to_categorical(Y_test)
print("Y_test :",Y_test.shape)

x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.2 , random_state = 109,shuffle=True)
x_train.shape, x_val.shape, y_train.shape, y_val.shape

"""**Model**"""

model = create_model()
model.summary()
adam_optim = Adam(learning_rate = 0.0001)
model.compile(optimizer=adam_optim, loss='categorical_crossentropy', metrics=['accuracy'])

X_train.shape, Y_train.shape

his = model.fit(
    X_train, Y_train,
    batch_size=2,
    epochs=500, 
    validation_data=(x_val, y_val)
)

"""**Plot & Evaluate**"""

def create_trace(x,y,ylabel,color):
        trace = go.Scatter(x = x,y = y,name=ylabel,marker=dict(color=color),mode = "markers+lines",text=x)
        return trace
    
def plot_accuracy_and_loss(train_model):
    hist = train_model.history
    acc = hist['accuracy']
    val_acc = hist['val_accuracy']
    loss = hist['loss']
    val_loss = hist['val_loss']
    epochs = list(range(1,len(acc)+1))
    
    trace_ta = create_trace(epochs,acc,"Training accuracy", "Green")
    trace_va = create_trace(epochs,val_acc,"Validation accuracy", "Red")
    trace_tl = create_trace(epochs,loss,"Training loss", "Blue")
    trace_vl = create_trace(epochs,val_loss,"Validation loss", "Magenta")
   
    fig = subplots.make_subplots(rows=1,cols=2, 
                                 subplot_titles=('Training and validation accuracy','Training and validation loss'))
    fig.append_trace(trace_ta,1,1)
    fig.append_trace(trace_va,1,1)
    fig.append_trace(trace_tl,1,2)
    fig.append_trace(trace_vl,1,2)
    fig['layout']['xaxis'].update(title = 'Epoch')
    fig['layout']['xaxis2'].update(title = 'Epoch')
    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])
    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])

    plotly.offline.iplot(fig, filename='accuracy-loss')

import plotly.graph_objs as go
from plotly import subplots
import plotly

plot_accuracy_and_loss(his)

score = model.evaluate(X_test,Y_test, verbose = 0)
print("Test Loss:: ",score[0])
print("Test Accuracy:: ",score[1])

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import pandas as pd

predicted_classes = model.predict(X_test)
predicted_classes = np.argmax(predicted_classes,axis=1)

y_true = np.argmax(Y_test,axis = -1)

y_true

cm = confusion_matrix(y_true, predicted_classes)
cm

import seaborn as sn
import matplotlib.pyplot as plt

df_cm = pd.DataFrame(cm, range(5), range(5))
plt.figure(figsize=(12,8))
sn.set(font_scale=1.3) # for label size
sn.heatmap(df_cm, annot=True, annot_kws={"size": 15}, fmt='g') # for num predict size

plt.show()

"""# **CNN**"""

from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Conv2D

from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.optimizers import Adam

file_name = 'drive/My Drive/dataset/TH/W/TH_W (1).wav'
audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') 

print(audio.shape, sample_rate)

mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
print(mfccs.shape)

librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time')

"""**extract feature**"""

max_pad_len = 1000

def extract_features(file_name):
    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') 
    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
    pad_width = max_pad_len - mfccs.shape[1]
    mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant') 
    return mfccs

"""**TH**"""

thw = dataset+"TH/W/TH_W (*).wav"

thm = dataset+"TH/M_w/TH_M (*).wav"

TH2_features = []

for file_name in Path().glob(thw):
    data = extract_features(file_name)
    
    TH2_features.append(data)

for file_name in Path().glob(thm):
    data = extract_features(file_name)
    
    TH2_features.append(data)

print(len(TH2_features))
TH2_features[0].shape

"""**JP**"""

jpw = dataset+"JP/W/JP_W (*).wav"
jpm = dataset+"JP/M/JP_M (*).wav"

JP2_features = []

for file_name in Path().glob(jpw):
    data = extract_features(file_name)
    
    JP2_features.append(data)

for file_name in Path().glob(jpm):
    data = extract_features(file_name)
    
    JP2_features.append(data)

print(len(JP2_features))
JP2_features[0].shape

"""**KR**"""

krw = dataset+"KR/W_w/KR_W (*).wav"
krm = dataset+"KR/M_w/KR_M (*).wav"

KR2_features = []

for file_name in Path().glob(krw):
    data = extract_features(file_name)
    
    KR2_features.append(data)

for file_name in Path().glob(krm):
    data = extract_features(file_name)
    
    KR2_features.append(data)

print(len(KR2_features))
KR2_features[0].shape

"""**CN**"""

cnw = dataset+"CN/W/CN_W (*).wav"
cnm = dataset+"CN/M/CN_M (*).wav"

CN2_features = []

for file_name in Path().glob(cnw):
    data = extract_features(file_name)
    
    CN2_features.append(data)

for file_name in Path().glob(cnm):
    data = extract_features(file_name)
    
    CN2_features.append(data)

print(len(CN2_features))
CN2_features[0].shape

"""**EN**"""

enw = dataset+"EN/W/EN_W (*).wav"
enm = dataset+"EN/M/EN_M (*).wav"

EN2_features = []

for file_name in Path().glob(enw):
    data = extract_features(file_name)
    
    EN2_features.append(data)

for file_name in Path().glob(enm):
    data = extract_features(file_name)
    
    EN2_features.append(data)

print(len(EN2_features))
EN2_features[0].shape

"""**Split data**

TH
"""

len(TH2_features)

yTH2 = [0 for i in range(200)]

XTH2_train,XTH2_test,YTH2_train,YTH2_test = train_test_split(TH2_features,yTH2,test_size=0.2,random_state=109,shuffle=True)

len(XTH2_train),len(XTH2_test),len(YTH2_train),len(YTH2_test)

"""**JP**"""

len(JP2_features)

yJP2 = [1 for i in range(200)]

XJP2_train,XJP2_test,YJP2_train,YJP2_test = train_test_split(JP2_features,yJP2,test_size=0.2,random_state=109,shuffle=True)

len(XJP2_train),len(XJP2_test),len(YJP2_train),len(YJP2_test)

"""**KR**"""

len(KR2_features)

yKR2 = [2 for i in range(200)]

XKR2_train,XKR2_test,YKR2_train,YKR2_test = train_test_split(KR2_features,yKR2,test_size=0.2,random_state=109,shuffle=True)

len(XKR2_train),len(XKR2_test),len(YKR2_train),len(YKR2_test)

"""**CN**"""

len(CN2_features)

yCN2 = [3 for i in range(200)]

XCN2_train,XCN2_test,YCN2_train,YCN2_test = train_test_split(CN2_features,yCN2,test_size=0.2,random_state=109,shuffle=True)

len(XCN2_train),len(XCN2_test),len(YCN2_train),len(YCN2_test)

"""**EN**"""

len(EN2_features)

yEN2 = [4 for i in range(200)]

XEN2_train,XEN2_test,YEN2_train,YEN2_test = train_test_split(EN2_features,yEN2,test_size=0.2,random_state=109,shuffle=True)

len(XEN2_train),len(XEN2_test),len(YEN2_train),len(YEN2_test)

"""**Combine**

X_train
"""

X2_train = np.vstack((XTH2_train,XJP2_train,XKR2_train,XCN2_train,XEN2_train))
print("X2_train :",X2_train.shape)

X2_train = X2_train.reshape(X2_train.shape[0], X2_train.shape[1], X2_train.shape[2], 1)
print(X2_train.shape)

"""X_test"""

X2_test = np.vstack((XTH2_test,XJP2_test,XKR2_test,XCN2_test,XEN2_test))
print("X2_test :",X2_test.shape)

X2_test = X2_test.reshape(X2_test.shape[0], X2_test.shape[1], X2_test.shape[2], 1)
print(X2_test.shape)

"""Y_train"""

Y2_train = YTH2_train+YJP2_train+YKR2_train+YCN2_train+YEN2_train

Y2_train = to_categorical(Y2_train)
print("Y2_train :",Y2_train.shape)

"""Y_test"""

Y2_test = YTH2_test+YJP2_test+YKR2_test+YCN2_test+YEN2_test

Y2_test = to_categorical(Y2_test)
print("Y2_test :",Y2_test.shape)

X2_train = X2_train.reshape(X2_train.shape[0], X2_train.shape[1], X2_train.shape[2], 1)
print(X2_train.shape)

x2_train, x2_val, y2_train, y2_val = train_test_split(X2_train, Y2_train, test_size = 0.2 , random_state = 109,shuffle=True)
x2_train.shape, x2_val.shape, y2_train.shape, y2_val.shape

"""**Model**"""

num_rows = 40
num_columns = 1000
num_channels = 1
num_batch_size = 10
epochs = 500

model = Sequential()
model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.2))

model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.2))

model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.2))

model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.2))
model.add(GlobalAveragePooling2D())

model.add(Dense(5, activation='softmax'))

model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

his = model.fit(X2_train, Y2_train, batch_size=num_batch_size, epochs=epochs, verbose=1,validation_data=(x2_val, y2_val))

score = model.evaluate(X2_test,Y2_test, verbose = 0)
print("Test Loss:: ",score[0])
print("Test Accuracy:: ",score[1])

predicted_classes = model.predict(X2_test)
predicted_classes = np.argmax(predicted_classes,axis = -1)

y_true = np.argmax(X2_test,axis = -1)

predicted_classes

cm = confusion_matrix(y_true, predicted_classes)
cm

CATEGORIES = ['TH','JP','KR','CN','EN']

report = classification_report(y_true, predicted_classes, target_names=CATEGORIES, digits=4)

print(report)